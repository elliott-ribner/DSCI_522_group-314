{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import altair as alt\n",
    "alt.data_transformers.enable('json')\n",
    "#alt.renderers.enable('notebook')\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from docopt import docopt\n",
    "from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading results from the output files\n",
    "evaluation_matrix = pd.read_csv(\"../results/accuracies.csv\")\n",
    "evaluation_matrix.iloc[:, 1:] = evaluation_matrix.iloc[:, 1:].applymap(lambda x: round(x, 3))\n",
    "#evaluation_matrix_base = pd.read_csv(\"../results_baseline//accuracies.csv\")\n",
    "#head = pd.read_csv(\"../results/head.csv\")\n",
    "summary=pd.read_csv(\"../results/num_describe.csv\", index_col=0).applymap(lambda x: int(x))\n",
    "test_accuracy = evaluation_matrix.iloc[0][2]\n",
    "test_accuracy_base = evaluation_matrix.iloc[0][1]\n",
    "recall = round(evaluation_matrix.iloc[2][2],2)\n",
    "recall_base = round(evaluation_matrix.iloc[2][1],2)\n",
    "precision = round(evaluation_matrix.iloc[3][2],2)\n",
    "precision_base = round(evaluation_matrix.iloc[3][1],2)\n",
    "auc = round(evaluation_matrix.iloc[4][2],2)\n",
    "auc_base = round(evaluation_matrix.iloc[4][1],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Table of Content:**\n",
    "* Summary\n",
    "* Introduction\n",
    "* Methods\n",
    "* Results\n",
    "* Conclusions\n",
    "* References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "auc": "0.72",
     "precision": "0.39",
     "recall": "0.63",
     "test_accuracy": "0.7"
    }
   },
   "source": [
    "# 1. Summary <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "In this project, we try to find the best features that best predict default customers using machine learning tools. Logistic Regression was found to achieve acceptable results on the test data provided to the trained model. The accuracy of the model on test data was about {{test_accuracy}} and the recall on test data found to be {{recall}}. The precision for the model on the test was about {{precision}} . The area under the ROC Curve for the final model is {{auc}}.\n",
    "\n",
    "Due to the risk associated with customers failing to pay, the model was designed to maximize the recall rate, identifying customers that will default to the greatest extent. This was also balanced with the overall accuracy of the training and test dataset. The model predicts the following 7 features to be the most important features to predict customers default.\n",
    "\n",
    "1. Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit.\n",
    "2. EDUCATION\n",
    "3. MARRIAGE\n",
    "4. AGE\n",
    "5. Past monthly repayment status in September 2005 (which is the most recent month before the prediction month)\n",
    "6. Past monthly repayment status in August 2005 (which is the second from the most recent month before the prediction month)\n",
    "7. Amount of previous payment (NT dollar) in September 2005 (which is the most recent month before the prediction month)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Introduction <a class=\"anchor\" id=\"second-bullet\"></a>\n",
    "Prediction of customers default behaviour is critically important in Risk Management by lenders. In particular,  there has been a significant interest in identifying features that are associated with the highest prediction power to reduce the overall lender's credit risk. In this study, we perform a data-informed analysis to build a model that can successfully capture features that predict default payment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Methods <a class=\"anchor\" id=\"third-bullet\"></a>\n",
    "## Data\n",
    "We used credit default data collected from the Taiwanese market in 2005. The Data Set is available from [UCI Machine Learning Repository Irvine, CA: University of California, School of Information and Computer Science](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients). The data that contains 23 features from 30,000 customers. was originally publicized by Chung Hua University of Taiwan and Tamkang University of Taiwan. Features include :\n",
    "\n",
    "- `LIMIT_BAL`: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit. \n",
    "- `SEX`: Gender(1 = male; 2 = female).\n",
    "- `EDUCATION`: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others). \n",
    "- `MARRIAGE`: Marital status (1 = married; 2 = single; 3 = others).  \n",
    "- `AGE`: Age (year).  \n",
    "- `PAY_1`, `PAY_2`, ..., `PAY_6`: Past monthly repayment status in September 2005, August 2005, ..., April 2005 respectively. ( -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.)  \n",
    "- `BILL_AMT1`, `BILL_AMT2`, ..., `BILL_AMT6`: Amount of bill statement (NT dollar) in September 2005, August 2005, ..., April 2005 respectively.  \n",
    "- `PAY_AMT1`, `PAY_AMT2`, ..., `PAY_AMT6`: Amount of previous payment (NT dollar) in September 2005, August 2005, ..., April 2005 respectively.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immediately after importing the data it was split into training and test data. Only 75% of the data was used to train the models and the test data was only used to obtain the test performance of the model on unseen data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we created a list for numeric and categorical features, below is the summary statistic of the training data. It showing mean, standard deviation, min, max etc. The bill amount, payment amount and credit limit ranges are roughly similar which are around 800,000. Interestingly, the median for the bill statement amounts is around 20,000, but the median for payment amounts is 2,000. Age ranges from 21 to 75 which is reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "variables": {
     "summary": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>LIMIT_BAL</th>\n      <th>AGE</th>\n      <th>BILL_AMT1</th>\n      <th>BILL_AMT2</th>\n      <th>BILL_AMT3</th>\n      <th>BILL_AMT4</th>\n      <th>BILL_AMT5</th>\n      <th>BILL_AMT6</th>\n      <th>PAY_AMT1</th>\n      <th>PAY_AMT2</th>\n      <th>PAY_AMT3</th>\n      <th>PAY_AMT4</th>\n      <th>PAY_AMT5</th>\n      <th>PAY_AMT6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>count</td>\n      <td>22500.000000</td>\n      <td>22500.000000</td>\n      <td>22500.00000</td>\n      <td>22500.000000</td>\n      <td>22500.000000</td>\n      <td>22500.000000</td>\n      <td>22500.000000</td>\n      <td>22500.000000</td>\n      <td>22500.000000</td>\n      <td>2.250000e+04</td>\n      <td>22500.000000</td>\n      <td>22500.000000</td>\n      <td>22500.000000</td>\n      <td>22500.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>mean</td>\n      <td>167229.763556</td>\n      <td>35.487022</td>\n      <td>50992.89800</td>\n      <td>48905.718978</td>\n      <td>46629.685644</td>\n      <td>42932.418844</td>\n      <td>39905.282444</td>\n      <td>38385.688222</td>\n      <td>5714.377733</td>\n      <td>5.848260e+03</td>\n      <td>5132.902667</td>\n      <td>4728.448311</td>\n      <td>4725.760978</td>\n      <td>5282.126533</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>std</td>\n      <td>129384.485693</td>\n      <td>9.182223</td>\n      <td>73064.68632</td>\n      <td>70748.066294</td>\n      <td>68376.985307</td>\n      <td>63802.950987</td>\n      <td>60135.853082</td>\n      <td>58733.428102</td>\n      <td>17078.235838</td>\n      <td>2.191690e+04</td>\n      <td>16892.473653</td>\n      <td>15430.720628</td>\n      <td>15138.455175</td>\n      <td>18506.384982</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>min</td>\n      <td>10000.000000</td>\n      <td>21.000000</td>\n      <td>-165580.00000</td>\n      <td>-69777.000000</td>\n      <td>-157264.000000</td>\n      <td>-170000.000000</td>\n      <td>-81334.000000</td>\n      <td>-339603.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>25%</td>\n      <td>50000.000000</td>\n      <td>28.000000</td>\n      <td>3565.75000</td>\n      <td>2928.000000</td>\n      <td>2577.000000</td>\n      <td>2313.000000</td>\n      <td>1711.750000</td>\n      <td>1190.000000</td>\n      <td>990.000000</td>\n      <td>8.000000e+02</td>\n      <td>390.000000</td>\n      <td>285.750000</td>\n      <td>238.000000</td>\n      <td>119.750000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>50%</td>\n      <td>140000.000000</td>\n      <td>34.000000</td>\n      <td>22169.00000</td>\n      <td>20859.000000</td>\n      <td>19889.000000</td>\n      <td>18855.500000</td>\n      <td>17875.000000</td>\n      <td>16715.000000</td>\n      <td>2100.000000</td>\n      <td>2.001000e+03</td>\n      <td>1800.000000</td>\n      <td>1500.000000</td>\n      <td>1500.000000</td>\n      <td>1500.000000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>75%</td>\n      <td>240000.000000</td>\n      <td>41.000000</td>\n      <td>66732.75000</td>\n      <td>63104.250000</td>\n      <td>59532.500000</td>\n      <td>53339.500000</td>\n      <td>49743.000000</td>\n      <td>48863.500000</td>\n      <td>5006.000000</td>\n      <td>5.000000e+03</td>\n      <td>4512.000000</td>\n      <td>4000.000000</td>\n      <td>4000.000000</td>\n      <td>4000.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>max</td>\n      <td>800000.000000</td>\n      <td>75.000000</td>\n      <td>746814.00000</td>\n      <td>743970.000000</td>\n      <td>855086.000000</td>\n      <td>616836.000000</td>\n      <td>587067.000000</td>\n      <td>568638.000000</td>\n      <td>873552.000000</td>\n      <td>1.227082e+06</td>\n      <td>889043.000000</td>\n      <td>621000.000000</td>\n      <td>426529.000000</td>\n      <td>528666.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>AGE</th>\n",
       "      <th>BILL_AMT1</th>\n",
       "      <th>BILL_AMT2</th>\n",
       "      <th>BILL_AMT3</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>22500</td>\n",
       "      <td>22500</td>\n",
       "      <td>22500</td>\n",
       "      <td>22500</td>\n",
       "      <td>22500</td>\n",
       "      <td>22500</td>\n",
       "      <td>22500</td>\n",
       "      <td>22500</td>\n",
       "      <td>22500</td>\n",
       "      <td>22500</td>\n",
       "      <td>22500</td>\n",
       "      <td>22500</td>\n",
       "      <td>22500</td>\n",
       "      <td>22500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>167229</td>\n",
       "      <td>35</td>\n",
       "      <td>50992</td>\n",
       "      <td>48905</td>\n",
       "      <td>46629</td>\n",
       "      <td>42932</td>\n",
       "      <td>39905</td>\n",
       "      <td>38385</td>\n",
       "      <td>5714</td>\n",
       "      <td>5848</td>\n",
       "      <td>5132</td>\n",
       "      <td>4728</td>\n",
       "      <td>4725</td>\n",
       "      <td>5282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>129384</td>\n",
       "      <td>9</td>\n",
       "      <td>73064</td>\n",
       "      <td>70748</td>\n",
       "      <td>68376</td>\n",
       "      <td>63802</td>\n",
       "      <td>60135</td>\n",
       "      <td>58733</td>\n",
       "      <td>17078</td>\n",
       "      <td>21916</td>\n",
       "      <td>16892</td>\n",
       "      <td>15430</td>\n",
       "      <td>15138</td>\n",
       "      <td>18506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10000</td>\n",
       "      <td>21</td>\n",
       "      <td>-165580</td>\n",
       "      <td>-69777</td>\n",
       "      <td>-157264</td>\n",
       "      <td>-170000</td>\n",
       "      <td>-81334</td>\n",
       "      <td>-339603</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>50000</td>\n",
       "      <td>28</td>\n",
       "      <td>3565</td>\n",
       "      <td>2928</td>\n",
       "      <td>2577</td>\n",
       "      <td>2313</td>\n",
       "      <td>1711</td>\n",
       "      <td>1190</td>\n",
       "      <td>990</td>\n",
       "      <td>800</td>\n",
       "      <td>390</td>\n",
       "      <td>285</td>\n",
       "      <td>238</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>140000</td>\n",
       "      <td>34</td>\n",
       "      <td>22169</td>\n",
       "      <td>20859</td>\n",
       "      <td>19889</td>\n",
       "      <td>18855</td>\n",
       "      <td>17875</td>\n",
       "      <td>16715</td>\n",
       "      <td>2100</td>\n",
       "      <td>2001</td>\n",
       "      <td>1800</td>\n",
       "      <td>1500</td>\n",
       "      <td>1500</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>240000</td>\n",
       "      <td>41</td>\n",
       "      <td>66732</td>\n",
       "      <td>63104</td>\n",
       "      <td>59532</td>\n",
       "      <td>53339</td>\n",
       "      <td>49743</td>\n",
       "      <td>48863</td>\n",
       "      <td>5006</td>\n",
       "      <td>5000</td>\n",
       "      <td>4512</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>800000</td>\n",
       "      <td>75</td>\n",
       "      <td>746814</td>\n",
       "      <td>743970</td>\n",
       "      <td>855086</td>\n",
       "      <td>616836</td>\n",
       "      <td>587067</td>\n",
       "      <td>568638</td>\n",
       "      <td>873552</td>\n",
       "      <td>1227082</td>\n",
       "      <td>889043</td>\n",
       "      <td>621000</td>\n",
       "      <td>426529</td>\n",
       "      <td>528666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       LIMIT_BAL    AGE  BILL_AMT1  BILL_AMT2  BILL_AMT3  BILL_AMT4  \\\n",
       "count      22500  22500      22500      22500      22500      22500   \n",
       "mean      167229     35      50992      48905      46629      42932   \n",
       "std       129384      9      73064      70748      68376      63802   \n",
       "min        10000     21    -165580     -69777    -157264    -170000   \n",
       "25%        50000     28       3565       2928       2577       2313   \n",
       "50%       140000     34      22169      20859      19889      18855   \n",
       "75%       240000     41      66732      63104      59532      53339   \n",
       "max       800000     75     746814     743970     855086     616836   \n",
       "\n",
       "       BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  \\\n",
       "count      22500      22500     22500     22500     22500     22500     22500   \n",
       "mean       39905      38385      5714      5848      5132      4728      4725   \n",
       "std        60135      58733     17078     21916     16892     15430     15138   \n",
       "min       -81334    -339603         0         0         0         0         0   \n",
       "25%         1711       1190       990       800       390       285       238   \n",
       "50%        17875      16715      2100      2001      1800      1500      1500   \n",
       "75%        49743      48863      5006      5000      4512      4000      4000   \n",
       "max       587067     568638    873552   1227082    889043    621000    426529   \n",
       "\n",
       "       PAY_AMT6  \n",
       "count     22500  \n",
       "mean       5282  \n",
       "std       18506  \n",
       "min           0  \n",
       "25%         119  \n",
       "50%        1500  \n",
       "75%        4000  \n",
       "max      528666  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 1. Summary the data used in this study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn the association between numeric features we explored their inter-correlations which can be seen below. \n",
    "We can observe that some features have a stronger co-linearity such as BILL-AMT1,BILL-AMT2,.. to BILL-AMT6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../results/num_corr_chart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1. Inter-correlation between numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also study the correlation between the features and the response variable. We can see that some of the features such as LIMIT_BALANCE and Age have a stronger correlation with the response variable than other features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../results/num_res_chart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[](roc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2. Correlation between numeric features and response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2 also shows that many of the features have a heavy tail distribution.  To mitigate this issue we applied [SMOTE](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html) (Synthetic Minority Oversampling Technique) on the response variable to create a balanced data set to fit the model. Furthermore, we implemented [`RobustScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) to scale predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Results <a class=\"anchor\" id=\"fourth-bullet\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We selected logistic regression model(`LogisticRegression`) and [`RFE`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE)(recursive feature elimination) as our model since it is more robust given that the dataset has many of the features are not normally distributed. One additional advantage of (`LogisticRegression`) that is much interpretable than more complex models\n",
    "\n",
    "We started the analysis by applying a robust scalar on the training data-set. Following that we build a model with the full set of features as our base-case model. The confusion matrix, evaluation matrix and ROC results were obtained to set a bench-mark for comparison purposes. `RFE` was then used to identify the most useful predictors and consequently we dropped those columns that are deemed as less useful. Eventually, 7 features were used to train the model.\n",
    "\n",
    "The hyperparameters `C` was tunned in the range from -4 to 20 using 5-fold cross-validation and the model was then fitted with the best hyperparameter. Let us now look at the result by glancing into the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best-case Model            |  Base-case Model\n",
    ":-------------------------:|:-------------------------:\n",
    "![](../results/confusion_matrix.png)  |  ![](../results_baseline/confusion_matrix.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 3. Confusion matrix of the fitted model with 7 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the best-case model which uses 7 features out-performs the base-case model in many aspects. First, it has obtained more true negatives on the test data than the base-case model. Furthermore, the best-case model has less false positives than the base-case model making it more precise as depicted in the evaluation matrix below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measurement</th>\n",
       "      <th>baseline</th>\n",
       "      <th>alternate model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test accuracy</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train accuracy</td>\n",
       "      <td>0.671</td>\n",
       "      <td>0.695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test recall</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test precision</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>auc score</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      measurement  baseline  alternate model\n",
       "0   test accuracy     0.670            0.700\n",
       "1  train accuracy     0.671            0.695\n",
       "2     test recall     0.665            0.625\n",
       "3  test precision     0.364            0.389\n",
       "4       auc score     0.721            0.715"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 2. Comparison of the evaluation matrix between models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "auc": "0.72",
     "precision": "0.39",
     "precision_base": "0.36",
     "recall": "0.63",
     "recall_base": "0.66",
     "test_accuracy": "0.7",
     "test_accuracy_base": "0.67"
    }
   },
   "source": [
    "Evaluation matrix in Table 2 shows the accuracy of the alternate model on test data was about {{test_accuracy}} compared with only {{test_accuracy_base}} for the baseline model. The recall on test data dropped slightly to {{recall}} compared with recall of the baseline which is {{recall_base}}. The precision for the model on the test data has improved {{precision}} compared with only {{precision_base}} for the baseline model. The area under the ROC Curve for the final model is {{auc}} which is comparable to the baseline model. ROC measures the model's discriminative ability. The dashed diagonal line represents a model that labels observations randomly. The further away from the diagonal line towards the top left corner, the better the model can distinguish two classes - default, non-default customers correctly.  The blue line is our best model with 7 features, we can see that the model has auc of {{auc}}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../results/roc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 4. ROC curve for the fitted model with 7 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "precision": "0.39",
     "precision_base": "0.36",
     "recall": "0.63",
     "recall_base": "0.66"
    }
   },
   "source": [
    "# 5. Conclusions <a class=\"anchor\" id=\"fifth-bullet\"></a>\n",
    "\n",
    "We were able to successfully use `LogisticRegression` model to find the most important features that predict customer default. The model was designed to maximize the recall rate, identifying customers that will default to the greatest extent. The model achieves an acceptable level of accuracy on the testing data, better tunning of hyperparameters may result in higher accuracy. Overall, we selected the best-case model to extract the most important features as it is more accurate. The precision of the best-case model is  {{precision}}. In comparison, the base-case model only scores  {{precision_base}}. While the recall of best-case model decreased from {{recall_base}} to {{recall}}, AUC score only slightly dropped.  Since the best-case model is more accurate, we expect the following 7 features to have the highest predictive power among all the features\n",
    "\n",
    "1. Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit.\n",
    "2. EDUCATION\n",
    "3. MARRIAGE\n",
    "4. AGE\n",
    "5. Past monthly repayment status in September 2005 (which is the most recent month before the prediction month)\n",
    "6. Past monthly repayment status in August 2005 (which is the second from the most recent month before the prediction month)\n",
    "7. Amount of previous payment (NT dollar) in September 2005 (which is the most recent month before the prediction month)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "test_accuracy": "0.7"
    }
   },
   "source": [
    "Although the best model is doing better than the baseline model overall, the result is still not very satisfactory with {{test_accuracy}} as our highest accuracy from test accuracy. To improve accuracy in the future, we have some suggestions that are not yet implemented due to time limitation. \n",
    "\n",
    "- Use some other feature scaling techniques: one-hot encoding the categorical features.\n",
    "- Use pipeline to be able to grid search the best combinations of values of LogisticRegression model parameters and number of features to select.\n",
    "- Use L1 regularization to eliminate features.  \n",
    "\n",
    "Another limitation is how to generalize the characteristics of feature 5,6,7. It makes sense the most months' repayment status and the amount would be a good indicator of whether the customer will default or not in the next month. However, how well can it predict if the customer will default or not in half a year or longer is explorabl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References <a class=\"anchor\" id=\"fifth-bullet\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. [UCI Machine Learning Repository Irvine, CA: University of California, School of Information and Computer Science](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)\n",
    "\n",
    "[2] [Guido Van Rossum and Fred L. Drake. Python 3 Reference Manual. CreateSpace, Scotts Valley, CA, 2009](https://dl.acm.org/doi/book/10.5555/1593511)\n",
    "\n",
    "[3] Wickham, H. 2017. tidyverse: Easily Install and Load the ‘Tidyverse’. R package version 1.2.1. https://CRAN.R-project.org/package=tidyverse\n",
    "\n",
    "[4] Wickham H (2011). “testthat: Get Started with Testing.” The R Journal, 3, 5–10. https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf.\n",
    "\n",
    "[5] McKinney, W. (2012). Python for data analysis: Data wrangling with Pandas, NumPy, and IPython. \" O'Reilly Media, Inc.\".\n",
    "\n",
    "[6] Nielsen, F. Å. (2014). Python programming—Scripting.\n",
    "\n",
    "[7] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., ... & Vanderplas, J. (2011). Scikit-learn: Machine learning in Python. Journal of machine learning research, 12(Oct), 2825-2830.\n",
    "\n",
    "[8] VanderPlas, J., Granger, B., Heer, J., Moritz, D., Wongsuphasawat, K., Satyanarayan, A., ... & Sievert, S. (2018). Altair: Interactive statistical visualizations for python. Journal of open source software, 3(32), 1057.\n",
    "\n",
    "[9] Percival, H. (2014). Test-driven development with Python: obey the testing goat: using Django, Selenium, and JavaScript. \" O'Reilly Media, Inc.\".\n",
    "\n",
    "[10] Lemaître, G., Nogueira, F., & Aridas, C. K. (2017). Imbalanced-learn: A python toolbox to tackle the curse of imbalanced datasets in machine learning. The Journal of Machine Learning Research, 18(1), 559-563.\n",
    "\n",
    "[11] Li, Susan. \"Building A Logistic Regression in Python, Step by Step.\" Towards Data Science (2017). https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[12] Yeh, I. C., & Lien, C. H. (2009). The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert Systems with Applications, 36(2), 2473-2480"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
